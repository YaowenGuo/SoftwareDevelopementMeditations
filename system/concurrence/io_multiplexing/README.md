# I/O Multiplexing

I/O 多路复用更通俗的叫法应该是：多路 I/O 复用一个线程的监听和处理（从而减少了线程的创建和销毁开销）。是实现一个线程可以监视多个文件句柄的同步I/O模型。I/O 操作是一个非常耗时的操作，在系统调用发出后，内核通过驱动像外设发出读取信号，数据没有到达之前，系统会将该线程挂起，继续执行其它线程，从而提高 CPU 的利用率。当数据到达后，内核才将线程的状态变为就绪状态，等待任务调度分配该线程继续执行时，I/O 函数才返回。由于 I/O 操作比较慢，我们有时候想要并发的进行多个 I/O 操作，其中任意一个 I/O 数据导到时，即可读取该 I/O 数据。**不同于线程，I/O 多路复用是 I/O 设备并发，而线程是 CPU 并发**

假设你要写一个 echo 服务，其能够同时响应用户从标准输入键入的交互指令。这种情况下，服务必须象想赢两个独立的 I/O 事件：(1)客户端发来的网络连接，(2)用户键盘输入的命令行。该优先等待哪一个事件？两种选择都不理想。当我们使用 accept 等待一个连接请求时，就无法响应输入的指令。同样的，当我们使用 read 等待用户输入时，就无法响应任何网络请求。

解决此困境的一种解决方案是一种称为 I/O 多路复用的技术，基本的思想是，告诉内核我们需要监听的 I/O，并被内核挂起，当监听的任一 I/O 状态改变时，内核将控制返还给应用。

由于历史原因，I/O 多路复用有不同的实现方案，这些实现的限制和性能各不相同，需要根据需要自己场景需要选择。

![Benchmark of I/O multiplexing](README_img/benchmark_of_io_multiplexing.jpeg)

select 和 poll 是 POSIX 标准中的函数，各个平台基本都支持，poll 解决了 select 监听数量上的限制（UNIX 上只能监听**前**1024个端口，windows 只能监听 64 个端口）。由于实现原理没有大的改变，两者性能基本一致。（**需要指出的是，虽然1024看起来挺多了，但由于文件、创建线程都会占用 FD, 前1024个FD很容易被耗尽，监听的 FD 大于 1024 就会出错。**）

epoll 是 Linux 2.6 引入的一个对 select 和 poll 性能改进的 I/O 多路复用函数。上面的的性能图可以看出，在 FD 大约 1000 时，epoll 性能更好，少于 1000 左右则 select 反而性能更好。实际使用还是根据场景需要选择。



## 基于 I/O 多路复用的事件驱动的并发服务

I/O多路复用可以用作并发事件驱动程序的基础，事件驱动中流根据事件而发生变化。一般的想法是将逻辑流建模为状态机。非正式地说，状态机是状态、输入事件和转换的集合，将状态和输入事件映射到状态。每个转换都将（输入状态、输入事件）对映射到输出状态。自循环是相同输入和输出状态之间的转换。状态机通常绘制为有向图，其中节点表示状态，有向弧线表示转换，弧线标签表示输入事件。状态机在某种初始状态下开始执行。每个输入事件都会触发从当前状态到下一个状态的转换。
