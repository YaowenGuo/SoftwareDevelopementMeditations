# Linux 虚拟内存的实现

Linux 没有使用不同数据结构分别描述进程和线程，而是使用 `task_struct` 数据结构统一描述一个执行的任务（这样看起来更加优雅，毕竟对于内核来说都是一个要调度的单位），其属性 `struct mm_struct  *mm` 表示该线程的地址空间，使用同一地址空间的 Task 属于同一进程。

- 指向同一个 mm 的 Task 属于同一进程。不同进程使用不同的 `struct mm_struct`。

- 内核线程对应的 task_struct 结构中的 mm 域指向 Null，所以内核线程之间调度是不涉及地址空间切换的。

当一个内核线程被调度时，它会发现自己的虚拟地址空间为 Null，而是直接复用上一个用户态线程的虚拟地址空间（相当于在当前用户线程所在的进程执行任务），这样做有很多好处，可以避免内核线程之间调度时地址空间的切换开销。最重要的是由于地址空间没有变化，避免了缓存失效，大大提高了运行速度。


**父进程与子进程的区别，进程与线程的区别，以及内核线程与用户态线程的区别其实都是围绕着这个 mm_struct 展开的。**


In the Linux kernel, init_mm is a special instance of the mm_struct structure that plays a crucial role in managing the memory space for the kernel. Here’s a detailed explanation of its role and features:

> 1. Initialization of init_mm
init_mm is the first mm_struct created during the kernel initialization process. It is used to manage the memory space of the kernel itself. The init_mm structure is statically initialized in the kernel source code and is used to set up the initial memory management context for the kernel.

> 2. Role of init_mm
- Kernel Memory Management: init_mm is used to manage the kernel’s address space. It contains information about the kernel’s virtual memory layout, including the page tables and memory regions.
- Global Page Directory: The pgd (page global directory) field of init_mm points to the global page directory (swapper_pg_dir). This page directory is used to map the kernel’s virtual addresses to physical addresses.
- Memory Initialization: During the kernel boot process, init_mm is used to initialize the kernel’s memory management subsystem. This includes setting up the initial page tables and memory regions.

> 3. Initialization Code
The init_mm structure is initialized in the mm/init-mm.c file. Here is a snippet of the initialization code:

```C
struct mm_struct init_mm = {
	.mm_mt		= MTREE_INIT_EXT(mm_mt, MM_MT_FLAGS, init_mm.mmap_lock),
	.pgd		= swapper_pg_dir,
	.mm_users	= ATOMIC_INIT(2),
	.mm_count	= ATOMIC_INIT(1),
	.write_protect_seq = SEQCNT_ZERO(init_mm.write_protect_seq),
	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
	.arg_lock	=  __SPIN_LOCK_UNLOCKED(init_mm.arg_lock),
	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
	.user_ns	= &init_user_ns,
	.cpu_bitmap	= CPU_BITS_NONE,
#ifdef CONFIG_IOMMU_SVA
	.pasid		= INVALID_IOASID,
#endif
INIT_MM_CONTEXT(init_mm)
};
```
This code sets up the initial state of init_mm, including the page directory pointer and various counters.

> 4. Setting Up Initial Memory Regions

The setup_initial_init_mm function is used to set up the initial memory regions for the kernel. This function is called during the kernel boot process and initializes fields such as start_code, end_code, end_data, and brk

```C
void setup_initial_init_mm(void *start_code, void *end_code,
void *end_data, void *brk)
{
	init_mm.start_code = (unsigned long)start_code;
	init_mm.end_code = (unsigned long)end_code;
	init_mm.end_data = (unsigned long)end_data;
	init_mm.brk = (unsigned long)brk;
}
```

> 5. Usage in Kernel Initialization
During the kernel boot process, init_mm is used to manage the kernel’s address space. This includes setting up the initial page tables and memory regions. The init_mm structure is used by the kernel to manage its own memory, ensuring that the kernel has a valid memory management context from the very beginning.

> Summary
init_mm is a crucial component of the Linux kernel’s memory management subsystem. It is used to manage the kernel’s address space, set up the initial page tables, and ensure that the kernel has a valid memory management context during the boot process. The init_mm structure is statically initialized and used to initialize the kernel’s memory management subsystem.


[进程管理中的active_mm是做什么的？](https://www.cnblogs.com/linhaostudy/p/18234020)

```C
struct mm_struct {
	struct {
		/*
		 * Fields which are often written to are placed in a separate
		 * cache line.
		 */
		struct {
			/**
			 * @mm_count: 自身引用计数。而 mm_users 是共享同一虚拟地址空间的线程数。
			 *
			 * Use mmgrab()/mmdrop() to modify. When this drops to
			 * 0, the &struct mm_struct is freed.
			 */
			atomic_t mm_count;
		} ____cacheline_aligned_in_smp;

		struct maple_tree mm_mt; // 基于范围的重叠树，用于管理虚拟内存区域（VMA）。替代了早期版本中的红黑树（mm_rb）。

		unsigned long mmap_base;	/* 新式布局下，内存映射区（mmap）的起始地址。*/
		unsigned long mmap_legacy_base;	/* 经典布局下，内存映射区（mmap）的起始地址。*/
#ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES
		/* Base addresses for compatible mmap() */
		unsigned long mmap_compat_base;
		unsigned long mmap_compat_legacy_base;
#endif
		unsigned long task_size;	/* size of task vm space */
		pgd_t * pgd; // 指向页全局目录（Page Global Directory），用于管理进程的页表。

#ifdef CONFIG_MEMBARRIER
		/**
		 * @membarrier_state: Flags controlling membarrier behavior.
		 *
		 * This field is close to @pgd to hopefully fit in the same
		 * cache-line, which needs to be touched by switch_mm().
		 */
		atomic_t membarrier_state;
#endif

		/**
		 * @mm_users: The number of users including userspace.
		 *
		 * Use mmget()/mmget_not_zero()/mmput() to modify. When this
		 * drops to 0 (i.e. when the task exits and there are no other
		 * temporary reference holders), we also release a reference on
		 * @mm_count (which may then free the &struct mm_struct if
		 * @mm_count also drops to 0).
		 */
		atomic_t mm_users;

#ifdef CONFIG_SCHED_MM_CID
		/**
		 * @pcpu_cid: Per-cpu current cid.
		 *
		 * Keep track of the currently allocated mm_cid for each cpu.
		 * The per-cpu mm_cid values are serialized by their respective
		 * runqueue locks.
		 */
		struct mm_cid __percpu *pcpu_cid;
		/*
		 * @mm_cid_next_scan: Next mm_cid scan (in jiffies).
		 *
		 * When the next mm_cid scan is due (in jiffies).
		 */
		unsigned long mm_cid_next_scan;
#endif
#ifdef CONFIG_MMU
		atomic_long_t pgtables_bytes;	/* size of all page tables */
#endif
		int map_count;			/* number of VMAs */

		spinlock_t page_table_lock; /* Protects page tables and some counters  */
		/*
		 * With some kernel config, the current mmap_lock's offset
		 * inside 'mm_struct' is at 0x120, which is very optimal, as
		 * its two hot fields 'count' and 'owner' sit in 2 different
		 * cachelines,  and when mmap_lock is highly contended, both
		 * of the 2 fields will be accessed frequently, current layout
		 * will help to reduce cache bouncing.
		 *
		 * So please be careful with adding new fields before
		 * mmap_lock, which can easily push the 2 fields into one
		 * cacheline.
		 */
		struct rw_semaphore mmap_lock;

		struct list_head mmlist; /* List of maybe swapped mm's.	These
					  * are globally strung together off
					  * init_mm.mmlist, and are protected
					  * by mmlist_lock
					  */
#ifdef CONFIG_PER_VMA_LOCK
		/*
		 * This field has lock-like semantics, meaning it is sometimes
		 * accessed with ACQUIRE/RELEASE semantics.
		 * Roughly speaking, incrementing the sequence number is
		 * equivalent to releasing locks on VMAs; reading the sequence
		 * number can be part of taking a read lock on a VMA.
		 *
		 * Can be modified under write mmap_lock using RELEASE
		 * semantics.
		 * Can be read with no other protection when holding write
		 * mmap_lock.
		 * Can be read with ACQUIRE semantics if not holding write
		 * mmap_lock.
		 */
		int mm_lock_seq;
#endif


		unsigned long hiwater_rss; /* High-watermark of RSS usage */
		unsigned long hiwater_vm;  /* High-water virtual memory usage */

		unsigned long total_vm;	   /* Total pages mapped */
		unsigned long locked_vm;   /* Pages that have PG_mlocked set */
		atomic64_t    pinned_vm;   /* Refcount permanently increased */
		unsigned long data_vm;	   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
		unsigned long exec_vm;	   /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
		unsigned long stack_vm;	   /* VM_STACK */
		unsigned long def_flags;

		/**
		 * @write_protect_seq: Locked when any thread is write
		 * protecting pages mapped by this mm to enforce a later COW,
		 * for instance during page table copying for fork().
		 */
		seqcount_t write_protect_seq;

		spinlock_t arg_lock; /* protect the below fields */

		unsigned long start_code, end_code, start_data, end_data;
		unsigned long start_brk, brk, start_stack;
		unsigned long arg_start, arg_end, env_start, env_end;

		unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

		struct percpu_counter rss_stat[NR_MM_COUNTERS];

		struct linux_binfmt *binfmt;

		/* Architecture-specific MM context */
		mm_context_t context;

		unsigned long flags; /* Must use atomic bitops to access */

#ifdef CONFIG_AIO
		spinlock_t			ioctx_lock;
		struct kioctx_table __rcu	*ioctx_table;
#endif
#ifdef CONFIG_MEMCG
		/*
		 * "owner" points to a task that is regarded as the canonical
		 * user/owner of this mm. All of the following must be true in
		 * order for it to be changed:
		 *
		 * current == mm->owner
		 * current->mm != mm
		 * new_owner->mm == mm
		 * new_owner->alloc_lock is held
		 */
		struct task_struct __rcu *owner;
#endif
		struct user_namespace *user_ns;  // 用户命名空间，保存限制资源使用的信息。

		/* store ref to file /proc/<pid>/exe symlink points to */
		struct file __rcu *exe_file;
#ifdef CONFIG_MMU_NOTIFIER
		struct mmu_notifier_subscriptions *notifier_subscriptions;
#endif
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
		pgtable_t pmd_huge_pte; /* protected by page_table_lock */
#endif
#ifdef CONFIG_NUMA_BALANCING
		/*
		 * numa_next_scan is the next time that PTEs will be remapped
		 * PROT_NONE to trigger NUMA hinting faults; such faults gather
		 * statistics and migrate pages to new nodes if necessary.
		 */
		unsigned long numa_next_scan;  // 下次 NUMA 扫描的时间。

		/* Restart point for scanning and remapping PTEs. */
		unsigned long numa_scan_offset; // NUMA 扫描的偏移量。

		/* numa_scan_seq prevents two threads remapping PTEs. */
		int numa_scan_seq; // NUMA 扫描序列号。
#endif
		/*
		 * An operation with batched TLB flushing is going on. Anything
		 * that can move process memory needs to flush the TLB when
		 * moving a PROT_NONE mapped page.
		 */
		atomic_t tlb_flush_pending; // 表示是否有 TLB 刷新操作待处理。
#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
		/* See flush_tlb_batched_pending() */
		atomic_t tlb_flush_batched; // 表示是否有批量 TLB 刷新操作。
#endif
		struct uprobes_state uprobes_state; // 用户探针状态。
#ifdef CONFIG_PREEMPT_RT
		struct rcu_head delayed_drop;
#endif
#ifdef CONFIG_HUGETLB_PAGE
		atomic_long_t hugetlb_usage;
#endif
		struct work_struct async_put_work; // 异步释放工作。

#ifdef CONFIG_IOMMU_MM_DATA
		struct iommu_mm_data *iommu_mm;
#endif
#ifdef CONFIG_KSM
		/*
		 * Represent how many pages of this process are involved in KSM
		 * merging (not including ksm_zero_pages).
		 */
		unsigned long ksm_merging_pages;
		/*
		 * Represent how many pages are checked for ksm merging
		 * including merged and not merged.
		 */
		unsigned long ksm_rmap_items;
		/*
		 * Represent how many empty pages are merged with kernel zero
		 * pages when enabling KSM use_zero_pages.
		 */
		atomic_long_t ksm_zero_pages;
#endif /* CONFIG_KSM */
#ifdef CONFIG_LRU_GEN_WALKS_MMU
		struct {
			/* this mm_struct is on lru_gen_mm_list */
			struct list_head list;
			/*
			 * Set when switching to this mm_struct, as a hint of
			 * whether it has been used since the last time per-node
			 * page table walkers cleared the corresponding bits.
			 */
			unsigned long bitmap;
#ifdef CONFIG_MEMCG
			/* points to the memcg of "owner" above */
			struct mem_cgroup *memcg;
#endif
		} lru_gen;
#endif /* CONFIG_LRU_GEN_WALKS_MMU */
	} __randomize_layout;

	/*
	 * The mm_cpumask needs to be at the end of mm_struct, because it
	 * is dynamically sized based on nr_cpu_ids.
	 */
	unsigned long cpu_bitmap[]; // 动态大小的 CPU 位图，用于跟踪哪些 CPU 可以访问该内存上下文。
};
```

## 虚拟内存分配

用户空间的虚拟内存分配只有两种系统调用。

1. brk 和 sbrk。
2. mmap、munmap、mremap。

所有用户空间的虚拟内存分配本质上都是映射。按映射的类型分为：

1. 匿名映射
2. 文件映射

包括 bar 和 sbrk 分配的堆内存也被归类为匿名映射。


在调用 mmap 进行匿名映射的时候（比如进行堆内存的分配），是将进程虚拟内存空间中的某一段虚拟内存区域与物理内存中的匿名内存页进行映射（嗯？不是只是分配虚拟内存空间吗？直到访问时发生缺页中断才映射物理内存。）当调用 mmap 进行文件映射的时候，是将进程虚拟内存空间中的某一段虚拟内存区域与磁盘中某个文件中的某段区域进行映射。
