# AudioEngine

## Audio 的处理流程：

![Audio Process Flow][1]

- [Device: 声音采集/播放设备及其 API.](10.1.audio_device.md)
- 3A算法：
    - AEC: 声学回声消除
    - ANS: 自适应噪声抑制
    - AGC: 自动增益控制

- 编解码
- JB: 抖动消除
- PLC: 丢包补偿


## 架构

AudioEngine 为例处理 Audio，将其划分为几个模块：
- ADM(Audio Device Moduel): 硬件的接口，负责采集和播放
- APM(Audio Process Moduel): 音频处理模块，混音和3A
- AudioSendStream/AudioReceiveStream:
- ChannelSend/ChannelReceive:
- RTP: RTP 封包，拥塞控制等


使用方式：

WebRtcVoiceEngine

```
1. 创建 WebRtcVoiceEngine， 可选传入 ADM
2. WebRtcVoiceEngine::Init() （创建 ADM、AudioState、AudioTransport 同时将 ADM 和 AudioTransport 建立关联)
3. 调用 WebRtcVoiceEngine::CreateMediaChannel 创建 WebRtcVoiceMediaChannel

4. WebRtcVoiceMediaChannel::AddSendStream 创建 WebRtcAudioSendStream（构造函数中创建 AudioSendStream）
```

### ADM

ADM 定义了硬件对外的接口，开始/停止采集音频，开始/停止播放。以及数据接口 `AudioDeviceBuffer`。
为了控制 ADM, VoiceEngine 中一个 `AudioState` 通过添加/移除 `AudioSendStream` 开启/关闭采集。或者也可通过 `SetRecording(bool enabled)` 接口开启录音，该接口是有 PC 的 `SetAudioRecording(bool recording)` 提供给用户的接口；播放与此类似。

创建：PCFactory 创建事传入，或者 `WebRtcVoiceEngine::Init()` 创建默认的。

### APM

创建：PCFactory 创建时传入，或者 `CreatePeerConnectionFactory` 中 AudioProcessingBuilder().Create() 创建默认的。

### AudioState & AudioTransport

为了管理 ADM，AudioState 对应了 ADM 的各种状态和控制。它在 `WebRtcVoiceEngine::Init` 中被创建。
在 AudioState 的构造函数中同时创建了 `AudioTransportImpl`。APM 被整合在 AudioTransport 中，负责各种音频处理。

```
+------------+                      +------------------+
| AudioState | --- 控制 ------->     |      ADM         |
+------------+                      +------------------+
                                            |     △
                                            ▽     |
                                    +--------------------+
                                    | AudioTransport     |
                                    |(AudioTransportImpl)|
                                    +--------------------+
                                       |            |
                          +-------------------+ +--------------------+
                          |  AudioSender      | | AudioReceiver      |
                          |(AudioSenderStream)| |(AudioReceiveStream)|
                          +-------------------+ +--------------------+


.
├── modules
|   ├── async_audio_processing // 预留给外部的处理接口
|   ├── audio_coding        // 编解码
│   ├── audio_device        // ADM 的接口定义
|   ├── audio_mixer         // 混音
|   ├── audio_processing    // 3A 处理
|   ...
├── sdk
│   ├── android              // 安卓的 AMD 实现
|   ...
├── audio                    // 对 Audio 发送的抽象，处理 AudioDevide 的状态，3A 处理，编解码。AudioState + AudioTransportImpl + ChannelSend/ChannelReceive + AudioSendStream/AudioReceiveStream
├── media
|   ├── engine              // 关联 Sender 和网络状态
|   ├──
```

关联流程：
```
AMD    创建 PCFactory 之前创建，或者在创建 VoiceEngine 的 init 中创建默认的。
    }-> WebRtcVoiceEngine::Init() 中 adm()->RegisterAudioCallback 建立关联。
AudioTransport 也是在 PCFactory 创建之前，WebRtcVoiceEngine::Init 时创建
    }-> AudioState 的 AudioState::AddSendingStream/RemoveSendingStream 添加或删除 Sender
AudioSender
    }-> 实现类 AudioSenderStream 创建的时候，同时创建 ChannelSend， RTPSenderAudio + ModuleRtpRtcpImpl2 + RTPSender
```

### AudioSender/AudioReceiver

AudioSender/AudioReceiver 对应一个发/收流，其在应用 SDP 的时候被创建（SdpOfferAnswerHandler::PushdownMediaDescription），最终在 `AudioState 的 AudioState::AddSendingStream/RemoveSendingStream` 中添加到 `AudioTransport`。

### WebRtcVoiceMediaChannel

VoiceChannel 算是 Stream 的发送和接收的整合，是一个双向流（BaseChannel 被删除后，不知道还能存在多久，毕竟通道的概念显得有些模糊）

创建 WebRtcVoiceMediaChannel

```C++
SdpOfferAnswerHandler::ApplyLocalDescription / SdpOfferAnswerHandler::RemoteDescriptionOperation
↓
SdpOfferAnswerHandler::CreateChannels(const SessionDescription& desc)
↓
SdpOfferAnswerHandler::CreateVoiceChannel
↓
cricket::WebRtcVoiceEngine::CreateMediaChannel // 使用 VoiceEngine 创建 WebRtcVoiceMediaChannel 和
cricket::VoiceChannel::VoiceChannel() 和 并使用 WebRtcVoiceMediaChannel 作为参数创建 VoiceChannel
创建的 VoiceChannel 都存在了 ChannelMananger 中。
↓
cricket::WebRtcVoiceMediaChannel::WebRtcVoiceMediaChannel // 构造函数
```


AudioSender(webrtc::internal::AudioSendStream::AudioSendStream) 的创建

```C++
webrtc::SdpOfferAnswerHandler::PushdownMediaDescription
↓
cricket::BaseChannel::SetLocalContent
↓
cricket::VoiceChannel::SetLocalContent_w
↓
cricket::BaseChannel::UpdateLocalStreams_w // BaseChannel 是 VoiceChannel 的基类 UpdateRemoteStreams_w 用于创建 AddRecvStream
↓
cricket::WebRtcVoiceMediaChannel::AddSendStream
↓
cricket::WebRtcVoiceMediaChannel::WebRtcAudioSendStream::WebRtcAudioSendStream() 构造器
↓
webrtc::internal::Call::CreateAudioSendStream
↓
webrtc::internal::AudioSendStream::AudioSendStream + ChannelSend + RTPSenderAudio + ModuleRtpRtcpImpl2 + RTPSender
```

ChannelSend 构造函数中创建 ModuleRtpRtcpImpl2， 而 RTPSender 在 ModuleRtpRtcpImpl2 的构造函数中创建（RtpSenderContext）
然后创建 RTPSenderAudio，并将 RTPSender 作为参数。


在 `AudioState::AddSendingStream` 添加 AudioSender 的时候其实就已经开始采集音频了。这时候其实也关联了 AudioSender，所以音频可以继续向下传递。但是 ICE 并没有准备好，所以并没有被发送。发送时会调用 rtp_sender_->SendingMedia 检查 `RTPSender.sending_media_` 的状态。

> 从 Capture -> 编码 ->

```
RTPSenderVideo::SendVideo
↓
// src/modules/rtp_rtcp/source/rtp_sender.h
检查 rtp_sender_->SendingMedia() // 如果(RTPSender)没有正在发送，则结束。
↓
RTPSenderVideo::LogAndSendToNetwork
```
**可以在  RTPSender::SetSendingMediaStatus 之前收到 VideoStreamEncoder::OnFrame**

一直到 ICE 连通网络后：

```
P2PTransportChannel::SwitchSelectedConnection
                    ↓ sig slot (SignalReadyToSend)
DtlsTransport::OnReadyToSend
                    ↓ sig slot (SignalReadyToSend)
RtpTransport::OnReadyToSend
                    ↓
BaseChannel::OnTransportReadyToSend
                    ↓
MediaChannel::OnReadyToSend(bool) (WebRtcVideoChannel 为例)
                    ↓
Call::SignalChannelNetworkState
Call::UpdateAggregateNetworkState
                    ↓
RtpTransportControllerSend::OnNetworkAvailability （MaybeCreateControllers()）
                    ↓
RtpVideoSender::OnNetworkAvailability
                    ↓
webrtc::RtpVideoSender::SetActiveModulesLocked  at rtp_video_sender.cc:506:16


webrtc::ModuleRtpRtcpImpl2::SetSendingStatus rtp_rtcp_impl2.cc:298:18 / ::SetSendingMediaStatus  at rtp_rtcp_impl2.cc:313:35

webrtc::RTCPSender::SetSendingStatus at rtcp_sender.cc:216:8 / webrtc::RTPSender::SetSendingMediaStatus at rtp_sender.cc:566:19
```

webrtc::RTPSender::SetSendingMediaStatus 修改了 RTPSender.sending_media_  的状态，此时编码才可以发送，


## WebRtcVoiceEngine

为了管理音频设备，WebRTC 抽象了 VoiceEngine 类，且是设备单例的。作为创建 ConnectionContext 的条件，其在创建 PCFactory 之前就创建了。

```C++
// src/media/engine/webrtc_media_engine.cc
std::unique_ptr<MediaEngineInterface> CreateMediaEngine(
    MediaEngineDependencies dependencies) {
  ...
  auto audio_engine = std::make_unique<WebRtcVoiceEngine>(
      dependencies.task_queue_factory, std::move(dependencies.adm),
      std::move(dependencies.audio_encoder_factory),
      std::move(dependencies.audio_decoder_factory),
      std::move(dependencies.audio_mixer),
      std::move(dependencies.audio_processing),
      dependencies.audio_frame_processor, trials);
  ...
}
```

## 初始化

其在创建 FCFactory 之前完成了初始化，调用关系
```
PeerConnectionFactory::Create()
↓
ConnectionContext::ConnectionContext()
↓
media_engine_->Init() // media_engine_ 就是 CompositeMediaEngine，最终调用 WebRtcVoiceEngine 的 Init()。VideoEngine 什么也不需要做，因为 Video 的开启和关闭由上层处理。
```

```C++
// src/media/engine/webrtc_voice_engine.cc
void WebRtcVoiceEngine::Init() {
  RTC_DCHECK_RUN_ON(&worker_thread_checker_);
  RTC_LOG(LS_INFO) << "WebRtcVoiceEngine::Init";

  // TaskQueue expects to be created/destroyed on the same thread.
  RTC_DCHECK(!low_priority_worker_queue_);
  low_priority_worker_queue_.reset(
      new rtc::TaskQueue(task_queue_factory_->CreateTaskQueue(
          "rtc-low-prio", webrtc::TaskQueueFactory::Priority::LOW)));

  // 收集编码器
  // Load our audio codec lists.
  RTC_LOG(LS_VERBOSE) << "Supported send codecs in order of preference:";
  send_codecs_ = CollectCodecs(encoder_factory_->GetSupportedEncoders());
  for (const AudioCodec& codec : send_codecs_) {
    RTC_LOG(LS_VERBOSE) << ToString(codec);
  }
  // 收集解码器
  RTC_LOG(LS_VERBOSE) << "Supported recv codecs in order of preference:";
  recv_codecs_ = CollectCodecs(decoder_factory_->GetSupportedDecoders());
  for (const AudioCodec& codec : recv_codecs_) {
    RTC_LOG(LS_VERBOSE) << ToString(codec);
  }

#if defined(WEBRTC_INCLUDE_INTERNAL_AUDIO_DEVICE)
  // No ADM supplied? Create a default one.
  if (!adm_) {
    adm_ = webrtc::AudioDeviceModule::Create(
        webrtc::AudioDeviceModule::kPlatformDefaultAudio, task_queue_factory_);
  }
#endif  // WEBRTC_INCLUDE_INTERNAL_AUDIO_DEVICE
  RTC_CHECK(adm());
  // 初始化 ADM
  webrtc::adm_helpers::Init(adm());

  // 创建 AudioState
  // Set up AudioState.
  {
    webrtc::AudioState::Config config;
    if (audio_mixer_) {
      config.audio_mixer = audio_mixer_;
    } else {
      config.audio_mixer = webrtc::AudioMixerImpl::Create();
    }
    config.audio_processing = apm_;
    config.audio_device_module = adm_;
    if (audio_frame_processor_)
      config.async_audio_processing_factory =
          rtc::make_ref_counted<webrtc::AsyncAudioProcessing::Factory>(
              *audio_frame_processor_, *task_queue_factory_);
    audio_state_ = webrtc::AudioState::Create(config);
  }

  // 关联了 ADM 和 audio_transport(), 这个 audio_transport 不是传输层的 transport。而是
  // ADM 对外的接口，其实现是 AudioTransportImpl
  // Connect the ADM to our audio path.
  adm()->RegisterAudioCallback(audio_state()->audio_transport());

  // 应用默认选项
  // Set default engine options.
  {
    AudioOptions options;
    options.echo_cancellation = true;
    options.auto_gain_control = true;
#if defined(WEBRTC_IOS)
    // On iOS, VPIO provides built-in NS.
    options.noise_suppression = false;
#else
    options.noise_suppression = true;
#endif
    options.highpass_filter = true;
    options.stereo_swapping = false;
    options.audio_jitter_buffer_max_packets = 200;
    options.audio_jitter_buffer_fast_accelerate = false;
    options.audio_jitter_buffer_min_delay_ms = 0;
    options.audio_jitter_buffer_enable_rtx_handling = false;
    bool error = ApplyOptions(options);
    RTC_DCHECK(error);
  }
  initialized_ = true;
}

// 应用音频处理的各种选项：3A，立体声，jitter_buffer 选项，高通滤波器。就是检查传入的各种参数是不
// 是支持，是不是有硬件支持，如果不支持会根据情况进行修改，然后才设置给 AP(Audio Processor)。
bool WebRtcVoiceEngine::ApplyOptions(const AudioOptions& options_in) {
  RTC_DCHECK_RUN_ON(&worker_thread_checker_);
  RTC_LOG(LS_INFO) << "WebRtcVoiceEngine::ApplyOptions: "
                   << options_in.ToString();
  AudioOptions options = options_in;  // The options are modified below.

  // Set and adjust echo canceller options.
  // Use desktop AEC by default, when not using hardware AEC.
  bool use_mobile_software_aec = false;

#if defined(WEBRTC_IOS)
  if (options.ios_force_software_aec_HACK &&
      *options.ios_force_software_aec_HACK) {
    // EC may be forced on for a device known to have non-functioning platform
    // AEC.
    options.echo_cancellation = true;
    RTC_LOG(LS_WARNING)
        << "Force software AEC on iOS. May conflict with platform AEC.";
  } else {
    // On iOS, VPIO provides built-in EC.
    options.echo_cancellation = false;
    RTC_LOG(LS_INFO) << "Always disable AEC on iOS. Use built-in instead.";
  }
#elif defined(WEBRTC_ANDROID)
  use_mobile_software_aec = true;
#endif

// Set and adjust gain control options.
#if defined(WEBRTC_IOS)
  // On iOS, VPIO provides built-in AGC.
  options.auto_gain_control = false;
  RTC_LOG(LS_INFO) << "Always disable AGC on iOS. Use built-in instead.";
#elif defined(WEBRTC_ANDROID)
#endif

#if defined(WEBRTC_IOS) || defined(WEBRTC_ANDROID)
  // Turn off the gain control if specified by the field trial.
  // The purpose of the field trial is to reduce the amount of resampling
  // performed inside the audio processing module on mobile platforms by
  // whenever possible turning off the fixed AGC mode and the high-pass filter.
  // (https://bugs.chromium.org/p/webrtc/issues/detail?id=6181).
  if (minimized_remsampling_on_mobile_trial_enabled_) {
    options.auto_gain_control = false;
    RTC_LOG(LS_INFO) << "Disable AGC according to field trial.";
    if (!(options.noise_suppression.value_or(false) ||
          options.echo_cancellation.value_or(false))) {
      // If possible, turn off the high-pass filter.
      RTC_LOG(LS_INFO)
          << "Disable high-pass filter in response to field trial.";
      options.highpass_filter = false;
    }
  }
#endif

  if (options.echo_cancellation) {
    // Check if platform supports built-in EC. Currently only supported on
    // Android and in combination with Java based audio layer.
    // TODO(henrika): investigate possibility to support built-in EC also
    // in combination with Open SL ES audio.
    const bool built_in_aec = adm()->BuiltInAECIsAvailable();
    if (built_in_aec) {
      // Built-in EC exists on this device. Enable/Disable it according to the
      // echo_cancellation audio option.
      const bool enable_built_in_aec = *options.echo_cancellation;
      if (adm()->EnableBuiltInAEC(enable_built_in_aec) == 0 &&
          enable_built_in_aec) {
        // Disable internal software EC if built-in EC is enabled,
        // i.e., replace the software EC with the built-in EC.
        options.echo_cancellation = false;
        RTC_LOG(LS_INFO)
            << "Disabling EC since built-in EC will be used instead";
      }
    }
  }

  if (options.auto_gain_control) {
    bool built_in_agc_avaliable = adm()->BuiltInAGCIsAvailable();
    if (built_in_agc_avaliable) {
      if (adm()->EnableBuiltInAGC(*options.auto_gain_control) == 0 &&
          *options.auto_gain_control) {
        // Disable internal software AGC if built-in AGC is enabled,
        // i.e., replace the software AGC with the built-in AGC.
        options.auto_gain_control = false;
        RTC_LOG(LS_INFO)
            << "Disabling AGC since built-in AGC will be used instead";
      }
    }
  }

  if (options.noise_suppression) {
    if (adm()->BuiltInNSIsAvailable()) {
      bool builtin_ns = *options.noise_suppression;
      if (adm()->EnableBuiltInNS(builtin_ns) == 0 && builtin_ns) {
        // Disable internal software NS if built-in NS is enabled,
        // i.e., replace the software NS with the built-in NS.
        options.noise_suppression = false;
        RTC_LOG(LS_INFO)
            << "Disabling NS since built-in NS will be used instead";
      }
    }
  }

  if (options.stereo_swapping) {
    RTC_LOG(LS_INFO) << "Stereo swapping enabled? " << *options.stereo_swapping;
    audio_state()->SetStereoChannelSwapping(*options.stereo_swapping);
  }

  if (options.audio_jitter_buffer_max_packets) {
    RTC_LOG(LS_INFO) << "NetEq capacity is "
                     << *options.audio_jitter_buffer_max_packets;
    audio_jitter_buffer_max_packets_ =
        std::max(20, *options.audio_jitter_buffer_max_packets);
  }
  if (options.audio_jitter_buffer_fast_accelerate) {
    RTC_LOG(LS_INFO) << "NetEq fast mode? "
                     << *options.audio_jitter_buffer_fast_accelerate;
    audio_jitter_buffer_fast_accelerate_ =
        *options.audio_jitter_buffer_fast_accelerate;
  }
  if (options.audio_jitter_buffer_min_delay_ms) {
    RTC_LOG(LS_INFO) << "NetEq minimum delay is "
                     << *options.audio_jitter_buffer_min_delay_ms;
    audio_jitter_buffer_min_delay_ms_ =
        *options.audio_jitter_buffer_min_delay_ms;
  }
  if (options.audio_jitter_buffer_enable_rtx_handling) {
    RTC_LOG(LS_INFO) << "NetEq handle reordered packets? "
                     << *options.audio_jitter_buffer_enable_rtx_handling;
    audio_jitter_buffer_enable_rtx_handling_ =
        *options.audio_jitter_buffer_enable_rtx_handling;
  }

  webrtc::AudioProcessing* ap = apm();
  if (!ap) {
    RTC_LOG(LS_INFO)
        << "No audio processing module present. No software-provided effects "
           "(AEC, NS, AGC, ...) are activated";
    return true;
  }

  webrtc::AudioProcessing::Config apm_config = ap->GetConfig();

  if (options.echo_cancellation) {
    apm_config.echo_canceller.enabled = *options.echo_cancellation;
    apm_config.echo_canceller.mobile_mode = use_mobile_software_aec;
  }

  if (options.auto_gain_control) {
    const bool enabled = *options.auto_gain_control;
    apm_config.gain_controller1.enabled = enabled;
#if defined(WEBRTC_IOS) || defined(WEBRTC_ANDROID)
    apm_config.gain_controller1.mode =
        apm_config.gain_controller1.kFixedDigital;
#else
    apm_config.gain_controller1.mode =
        apm_config.gain_controller1.kAdaptiveAnalog;
#endif
  }

  if (options.highpass_filter) {
    apm_config.high_pass_filter.enabled = *options.highpass_filter;
  }

  if (options.noise_suppression) {
    const bool enabled = *options.noise_suppression;
    apm_config.noise_suppression.enabled = enabled;
    apm_config.noise_suppression.level =
        webrtc::AudioProcessing::Config::NoiseSuppression::Level::kHigh;
    RTC_LOG(LS_INFO) << "NS set to " << enabled;
  }

  ap->ApplyConfig(apm_config);
  return true;
}
```

## 创建编码器

BaseChannel 对外提供了接口，但是其职责是网络发送，最终还是要调用 MediaChannel 中的方法来创建。

```
https://www.freesion.com/article/68901420409/
```


[1]: ./images/audio_process_flow.png
参考：
https://blog.csdn.net/boywgw/article/details/48311987