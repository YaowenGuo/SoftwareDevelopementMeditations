# Video Module

由于 Video 的编解码更依赖于硬件，各个平台实现上差别较大。Vodeo 模块的安卓平台的实现是对 Java 的封装，Video 模块的创建和初始化流程大致可以概括为：

1. 创建 MediaEngine : 在创建 PCFactory 之前就需要创建 MediaEngine(调用 CreateModularPeerConnectionFactory 创建 PCFactory 之前。)
    1. 创建 webrtc::InternalEncoderFactory 和 webrtc::InternalDecoderFactory
    2. 创建 WebRtcVideoEngine 

2. 媒体协商时创建接收的 Sink 和 发送数据的 Source:
    1. 创建用于接收 Video 的 VideoSource，包括远端和本地的。: webrtc::JavaToNativeVideoSink
    2. 创建 VideoSource 用于协商发送 Video: webrtc::CreateJavaVideoSource 创建 AndroidVideoTrackSource。同时会创建 JNI 调用的 NativeAndroidVideoTrackSource Java 对象。
        在 这时候就创建了 NativeCapturerObserver。并将 Native 的 AndroidVideoTrackSource 作为参数传进构造函数了。

3. 在协商过程中设置 MediaEngine 的编解码器

4. 开启视频
    1. 从 VideoSource 获取 NativeCapturerObserver。
    2. 创建 VideoCapturer。调用 initialize 将 NativeCapturerObserver 传进去。
    3. 调用 videoCapturer.startCapture 开启视频。此时视频就由 VideoCaptur -> NativeCapturerObserver -> AndroidVideoTrackSource 发送数据了。


## VideoCaputure 模块的封装

可以看到，Video 的 Capture 基本是一个完全独立的模块。其可以在完全自主创建和操作，只是由一个 NativeCapturerObserver 将数据发送到 Source 即可。

1. 创建一个 CameraEnumerator, Android 5.0 开始可以直接使用 Camera2Enumerator.
2. 调用 Camera2Enumerator 的 createCapturer 创建一个 Camera2Capturer。
3. 开启视频
    1. 创建一个 VideoSource(应该和 PC 设置的 VideoSource 是绑定的), 从 VideoSource 获取 NativeCapturerObserver。
    2. 创建 VideoCapturer。调用 initialize 将 NativeCapturerObserver 传进去。
    3. 调用 videoCapturer.startCapture 开启视频。
    
此时视频就由 VideoCaptur -> NativeCapturerObserver -> AndroidVideoTrackSource 发送数据了。


- CameraEnumerator 就是一个用于查询设备摄像头信息的包装类。能够获取设备数量和前后置摄像头。
- CameraCapturer(Camera2Capturer) 才是封装了摄像头操作的各种事件和状态处理，实际操作摄像头交给 CameraSession。
- CameraSession 封装了摄像头的操作。其中 CameraStateCallback 承接了开启摄像头的各种回调。


1. VideoCapture 的数据输出到 Surface 中，WebRTC 使用 Surface 中的 SurfaceTexture 作为真正的数据接收对象。在 SurfaceTextureHelper 中创建了 SurfaceTexture 对象，并通过 `surfaceTexture.setOnFrameAvailableListener` 注册了数据监听。

2. tryDeliverTextureFrame 用于获取数据，生成 WebRTC 中的 VideoFrame.

3. 生成的 VideoFrame 通过 VideoSink::onFrame 传递给 WebRTC 的 Sink, 也就是 Camera2Session 调用 `surfaceTextureHelper.startListening` 注册的 Listene。

4. 然后调用 events.onFrameCaptured 将数据继续向下传递。events 其实是 CameraCapturer 中 `cameraSessionEventsHandler` 对象。

5. 然后调用 capturerObserver.onFrameCaptured，capturerObserver 就是 VideoCapturer.initialize 初始化中传递进来的 `NativeCapturerObserver`。NativeAndroidVideoTrackSource.onFrameCaptured 会继续调用 NativeAndroidVideoTrackSource.onFrameCaptured 将数据向下传递。然后调用 native 方法 nativeOnFrameCaptured.

根据 WebRTC 生成 JNI 函数的对应规则（生成的函数名为规则为 `JNI_下划线分割的包名_类名_函数名`），其会调用第一个参数的 Native 对象的 OnFrameCaptured，即 AndroidVideoTrackSource::OnFrameCaptured 方法。


## 预览和显示

Track 代表一个音/视频轨，是一个单向的数据流。Video 的对应实现是使用 ` PeerConnectionFactory::CreateVideoTrack` 创建的`VideoTrack`。

上面的 `AndroidVideoTrackSource` 就是 VideoTrack 的 Source，实现了 `VideoTrackSourceInterface`。

如果想要预览，可以调用 VideoTrack 的 `AddOrUpdateSink` 方法添加一个 Sink，用于接收本地的采集的视频流。Sink 是实现 VideoSinkInterface 接口的类，具体该如何显示根据不同平台有不同实现。如果要发送到远端，也会在 VideoTrack 中添加发送的 VideoSinkInterface 的实现。可以在 VideoSinkInterface 的 OnFrame 中接收视频帧用于显示。

```
            ┌--------Sink
Source --> Track --> Sink
            ├--------Sink
            ├         ...
```

- 一个 Track 对应一个音频或者视频流，是流就有输入和输出， Source 是输入，Sink 是输出。一个 Track 只能一个输入，但是有多个输出，比如，一个摄像头的输出既要发送到到远端，又要显示的屏幕预览，则一个 Sink 对应于发送，一个 Sink 对应于屏幕预览。

- 多个媒体对应多个 Track, 例如音频和视频各都应一个 Track。


Source 作为数据源，可以有摄像头、麦克风、文件、屏幕采集、远端接收等。以 P2P 通话为例，一个 Souce 用于屏幕采集用于发送，发送方发送的一个 track，在接收端也一定表现为一个 track。

一个 Souce 接收数据。给 track 加 sink 实际上都是给 track 的 source 加 sink。

音频和视频时独立的 Track, 同一个同步的音视频 Track 抽象为 MediaStream. 一个 track 可以归属于一个或多个 MediaStream，通过 stream id 区分，如果接收端尚不存在对应 stream，则会被创建。（rfc8829 4.1.2）

加一个 stream 是为了更灵活的控制本地和远端的 track 组合，比如本地有音视频，但只发送音频。其实直接指定发送哪些 track 也能达到这个效果，但是有多个音视频发送时，如何区分哪个音频对应哪个视频呢？对应的音频和视频也就是 Stream 的概念。

PC 是 PeerConnection 的简称，它表示了终端之间的 P2P 连接，用来传输数据。PC 是 WebRTC 的门面，我们直接使用的都是 PC 的接口。

总结下这四个概念的关系：PC - 一到多个 stream - 零到多个 track - 一个 source和一到多个Sink。


```
                        MediaSourceInterface
                                ^
                                |
                            Notifier
                                |
            ┌-------------------┴------------------┐------------------------┐
rtc::AdaptedVideoTrackSource        webrtc::VideoTrackSource        webrtc::LocalAudioSource
```

VideoSinkInterface
        ^
        |
FrameCadenceAdapterInterface
        ^
        |
FrameCadenceAdapterImpl

FrameCadenceAdapterImpl::OnFrame > FrameCadenceAdapterImpl::OnFrameOnMainQueue > ZeroHertzAdapterMode::OnFrame >

VideoStreamEncoder::CadenceCallback::OnFrame > VideoStreamEncoder::OnFrame

## 编解码



## 发送
