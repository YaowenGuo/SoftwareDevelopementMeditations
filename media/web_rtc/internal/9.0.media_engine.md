# Media Engine

MediaEngine 用于管理硬件数据流。MediaEngine 的实现 CompositeMediaEngine 是一个聚合类，包含一个 WebRtcVoiceEngine 和 一个 WebRtcVideoEngine（后文称其为 VoiceEngien 和 VideoEngine）。

VoiceEngien/VideoEngine 提供了如下的能力：

- 查询本机支持的编解码。
- 创建 Channel。


WebRtcVoiceEngine 和 WebRtcVideoEngine 用于创建 Channel(数据通道)，

> 协商中的类和传输中的类之间的对应关系

```
     协商                                                `传输`
RtpSenderBase          <------->  ChannelSend / RtpVideoSender
Audio/VideoRtpReceiver <------->  ChannelReceive / RtpVideoStreamReceiver2
                                          ╭-- 一个 ->  voe::ChannelSend {ModuleRtpRtcpImpl2 {RTPSender}, RTPSenderAudio }
                                        ╭-- 一个 ->   webrtc::AudioSendStream
                                      ╭-- 多个 ->  WebRtcAudioSendStream 和 WebRtcAudioReceiveStream
                                 ╭- 一个 -> WebRtcVoiceMediaChannel / WebRtcVideoChannel
RtcTranscervier        <------>  BaseChannel(VoiceChannel/VideoChannel)
```



一个 BaseChannel 包含一个 WebRtcVoiceMediaChannel/WebRtcVideoChannel。

Channel 和 Stream 的创建流程
```
SdpOfferAnswerHandler::CreateChannels 创建 BaseChannel 的子类
↓
RtpTransceiver::CreateChannel/RtpTransceiver::SetChannel()
↓
WebRtcVoiceEngine::CreateMediaChannel/WebRtcVideoEngine::CreateMediaChannel 创建 (WebRtcVoiceMediaChannel 和 WebRtcVideoChannel) 作为参数

创建 Stream
WebRtcVoiceMediaChannel::WebRtcAudioSendStream
↓
DegradedCall::CreateAudioSendStream
↓
Call::CreateAudioSendStream
↓
new AudioSendStream()
```

## Channel


通过 VoiceEngien/VideoEngine 的 `CreateMediaChannel` 来创建一个 `Channel`。

Channel 的实现使用了 Bridge 模式，用于实现上层 Media 和下层网络传输的桥接。

上层的 MediaChannel 是对媒体数据封装，实现是 WebRtcVoiceMediaChannel 和 WebRtcVideoChannel，其两者实现了 Transport 接口，也可以认为是传输层的入口。

下层的 BaseChannel (正在被删除，功能合并到 RtpTransceiver 中)实现了 MediaChannel 的 NetworkInterface 接口，用于发送数据，为了实现 Voice 和 Video 的差异部分。BaseChannel 实现了 VoiceChannel 和 VideoChannel 两个子类。

由于是双向流，BaseChannel 也持有上层 MediaChannel 的引用，用于接收数据的返回。


## Stream

Stream 是 WebRTC 中数据流的抽象，是一个单向流。通过 MediaChannel 的 `AddSendStream` 和 `AddRecvStream` 可以创建一个发送/接收流。也可以通过 `RemoveSendStream` 和 `RemoveRecvStream` 移除流。

Stream 的实现也有好几个类。

AddSendStream/AddRecvStream 创建的 `WebRtcAudioSendStream/WebRtcVideoSendStream` 和 `WebRtcAudioReceiveStream/WebRtcVideoReceiveStream` 其实是对上层的实现的封装。是一种适配器。

- WebRtcAudioSendStream 和 WebRtcAudioReceiveStream 分别是对 webrtc::AudioSendStream 和 webrtc::AudioReceiveStreamImpl。两者都是通过 Call 的 Create<XXX> 来创建的，例如 `Call::AudioSendStream`。

- WebRtcVideoSendStream 和 WebRtcVideoReceiveStream 分别是 VideoSendStream 和 VideoReceiveStream2 的封装。两者也是通过 Call::Create<XXX> 创建的。

可以看到 Stream 上层的实现类名字差别，没有什么规律。通过 Engein 中类的适配，上层能够快速迭代而不影响下层。通过这种方式将修改隔离。之所以上层 Stream 有这么多名字，而不是直接通过修改代码，很大一部分是因为 WebRTC 是开源提供给不同的项目使用的。有时候会需要测试新的实现的性能和影响，同时又不能影响都其它项目。


## 设置编解码器

设计媒体相关的参数，例如编解码器，都是通过 BaseChannel 的对外接口，最终调用 MeidiaChannel 完成的。

媒体协商确定编解码器，而 BaseChannel 是对外的接口，因此在处理 SDP 的 OfferAnswerHanler 中调用 BaseChannel 的 SetSendParameter() 中创建编解码器。


## Video 模块和 Audio 模块的差别。

在直观上 Video 总是和 Audio 对应，应该有一样的架构设计，然而现实是他们差距如此之大：获取的 Video 一般需要预览同时用于发送，然而采集的 Audio 却只会发送不可能在本地播放，这会引起回声（播放的声音被再次录音）（有没有本次存储的用例呢？）。 Video 可能来自摄像头、也可能来自屏幕录制。

Video 和 Audio 硬件接口的不同：

- Audio 的管理由 WebRTC 控制。Video 则需要用户控制打开和关闭。

- Audio 没有预览，是一个单向流，没有设计 Source 和 Sink。Video 有预览，数据流需要分叉，因此设计了 Source/Sink 可以添加多个 Sink.

- WebRtcVoiceEngine 创建需要一个 ADM，该接口封装了 Audio 的设备操作。而 Video 只需要编解码的 Fatory 即可。

一言蔽之：**Audio 和 Video 除了时间，没有任何性质相同。就像一条铁路线上的两条铁轨，同时而永远不会相交**在 WebRTC 上的反应就是共用网络发送层。其余各个模块都需要单独设计。