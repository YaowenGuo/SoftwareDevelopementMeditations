# Audio Device

ADM(AudioDeviceModule) 定义了 Audio 相关的的硬件接口，包括声音采集和播放。安卓的音频相关代码有两套：一套在 `modules/audio_device/android` 目录下，这部分是旧的，为了兼容老的 API；另一套在 `sdk/android` 目录下。如果在创建 `WebRtcVoiceEngine` 时，提供了参数 `webrtc::AudioDeviceModule` 则使用的是新代码，否则会在 `WebRtcVoiceEngine::Init` 中使用 `webrtc::AudioDeviceModule::Create` 创建老的。旧的 API 最终会被删掉，这里仅分析新的实现。

Audio 的硬件操作架构：

```
+-------------------------+--------------------------+
| ADM(AudioDeviceModule)  | AndroidAudioDeviceModule |
+-------------------------+--------------------------+
|            |            | AudioRecordJni | AudioTrackJni |
| AudioInput | AudioOutput| OpenSLESRecorder | OpenSLESPlayer |
|            |            | AAudioRecorder | AAudioPlayer |
+-------------------------+--------------------------+
|    数据传输              |      AudioDeviceBuffer   |
+-------------------------+--------------------------+
```

ADM 包括了采集和播放的 API，这两个其实是完全独立的功能，在实现中又定义了 AudioInput 和 AudioOutput 分别代表输出和输出。可以调用 ADM 的 `RegisterAudioCallback` 注册一个 ` AudioTranspor`：

-  ADM 的数据调用 `AudioTranspor::RecordedDataIsAvailable` 返回采样的数据；
- 调用 `AudioTranspor::NeedMorePlayData` 获取要播放的数据。


sdk 目录下 ADM 实现是 `AndroidAudioDeviceModule`, 同时抽象了输出设备 `AudioInput`, 输出设备 `AudioOutput`，三种实现方式分别实现为：

- AAudio:  C++ 接口，这种方式是 [Android O(8.0) 开始提供的 C/C++ 音频访问接口](https://developer.android.google.cn/ndk/guides/audio/aaudio/aaudio.html)。
    - AudioInput: AAudioRecorder
    - AudioOutput: AAudioPlayer
- JNI: 调用 Java 接口访问硬件。也是使用 SDK 是的访问方式。
    - AudioInput: AudioRecordJni
    - AudioOutput: AudioTrackJni
- OpenSLES: 直接访问安卓平台的底层 API 控制硬件。
    - AudioInput: OpenSLESRecorder
    - AudioOutput: OpenSLESPlayer


如果是在 Native 中使用，WebRTC 在 `sdk/android/native_api/audio_device_module/audio_device_android.cc` 中提供了创建三种不同类型接口的方式，你可以在创建 PCFactory 通过参数来自定义使用那种类型。而在 WebRTC 提供的 Android 的 SDK 中，在 Java 中使用 JNI 的方式创建。无论那种方式，最终都是调用 `CreateAudioDeviceModuleFromInputAndOutput`

Android 提供的是 Java SDK，使用的是 JNI 方式，其在 `PeerConnectionFactory:Builder.createPeerConnectionFactory()` 中初始化。

```Java
// PeerConnectionFactory.Builder
public PeerConnectionFactory createPeerConnectionFactory() {
  checkInitializeHasBeenCalled();
  if (audioDeviceModule == null) {
    audioDeviceModule = JavaAudioDeviceModule.builder(ContextUtils.getApplicationContext())
                            .createAudioDeviceModule(); // 创建了 Java 的 JavaAudioDeviceModule
  }
  return nativeCreatePeerConnectionFactory(ContextUtils.getApplicationContext(), options,
      audioDeviceModule.getNativeAudioDeviceModulePointer(),  // 创建了 C++ 的 AndroidDeviceModule
      ...);
}
```

```Java
public long getNativeAudioDeviceModulePointer() {
    synchronized(this.nativeLock) {
        if (this.nativeAudioDeviceModule == 0L) {
            // 调用 JNI 创建 C++ 的 AndroidDeviceModule
            this.nativeAudioDeviceModule = nativeCreateAudioDeviceModule(this.context, this.audioManager, this.audioInput, this.audioOutput, this.inputSampleRate, this.outputSampleRate, this.useStereoInput, this.useStereoOutput);
        }

        return this.nativeAudioDeviceModule;
    }
}

// 对应的 JNI 函数为，WebRTC 使用 脚本生成 JNI 调用，生成的函数名为规则为 `JNI_下划线分割的包名_类名_函数名`，生成的函数会直接调用
// 函数名为 `JNI_类名_函数去掉 native 前缀`。我们阅读代码时可以直接跳过生成的代码，查找对应的 Native 的函数即可。
// 例如 `JavaAudioDeviceModule.nativeCreateAudioDeviceModule` 对应调用的函数为 `JNI_JavaAudioDeviceModule_CreateAudioDeviceModule`.
```

```C++
// sdk/android/src/jni/audio_device/java_audio_device_module.cc
// 其就是在 PeerConnectionFactory:Builder.createPeerConnectionFactory() 中的 audioDeviceModule.getNativeAudioDeviceModulePointer() 被调用的。
static jlong JNI_JavaAudioDeviceModule_CreateAudioDeviceModule(
    JNIEnv* env,
    const JavaParamRef<jobject>& j_context,
    const JavaParamRef<jobject>& j_audio_manager,
    const JavaParamRef<jobject>& j_webrtc_audio_record,
    const JavaParamRef<jobject>& j_webrtc_audio_track,
    int input_sample_rate,
    int output_sample_rate,
    jboolean j_use_stereo_input,
    jboolean j_use_stereo_output) {
  AudioParameters input_parameters;
  AudioParameters output_parameters;
  GetAudioParameters(env, j_context, j_audio_manager, input_sample_rate,
                     output_sample_rate, j_use_stereo_input,
                     j_use_stereo_output, &input_parameters,
                     &output_parameters);
  auto audio_input = absl::make_unique<AudioRecordJni>(
      env, input_parameters, kHighLatencyModeDelayEstimateInMilliseconds,
      j_webrtc_audio_record);
  auto audio_output = absl::make_unique<AudioTrackJni>(env, output_parameters,
                                                       j_webrtc_audio_track);
  return jlongFromPointer(CreateAudioDeviceModuleFromInputAndOutput(
                              AudioDeviceModule::kAndroidJavaAudio,
                              j_use_stereo_input, j_use_stereo_output,
                              kHighLatencyModeDelayEstimateInMilliseconds,
                              std::move(audio_input), std::move(audio_output))
                              .release());
}
```


在内部实现上，注册的 `AudioTranspor` 实际注册给了 `AudioDeviceBuffer`， 采集的数据和需要播放的数据都需要和 `AudioDeviceBuffer` 交互获得。


CreateAudioDeviceModuleFromInputAndOutput 只是简单调用了 `AndroidAudioDeviceModule` 的构造方法。


## AMD 的初始化

WebRtcVoiceEngine::Init() 中将 ADM 初始化

```C++
src/media/engine/webrtc_voice_engine.cc
void WebRtcVoiceEngine::Init() {
  ...
  RTC_CHECK(adm());
  webrtc::adm_helpers::Init(adm());

  // Set up AudioState.
  {
    webrtc::AudioState::Config config;
    if (audio_mixer_) {
      config.audio_mixer = audio_mixer_;
    } else {
      config.audio_mixer = webrtc::AudioMixerImpl::Create();
    }
    config.audio_processing = apm_;
    config.audio_device_module = adm_;
    if (audio_frame_processor_)
      config.async_audio_processing_factory =
          rtc::make_ref_counted<webrtc::AsyncAudioProcessing::Factory>(
              *audio_frame_processor_, *task_queue_factory_);
    audio_state_ = webrtc::AudioState::Create(config);
  }

  // Connect the ADM to our audio path.
  adm()->RegisterAudioCallback(audio_state()->audio_transport());

  ...
}

// src/media/engine/adm_helpers.cc
void Init(AudioDeviceModule* adm) {
  RTC_DCHECK(adm);

  RTC_CHECK_EQ(0, adm->Init()) << "Failed to initialize the ADM.";

  // Playout device.
  {
    // Android 的 Device ID 是在 Java 中设置的，默认返回 true.
    if (adm->SetPlayoutDevice(AUDIO_DEVICE_ID) != 0) {
      RTC_LOG(LS_ERROR) << "Unable to set playout device.";
      return;
    }
    // 在 Init 中已经设置过了，返回 true.
    if (adm->InitSpeaker() != 0) {
      RTC_LOG(LS_ERROR) << "Unable to access speaker.";
    }

    // false 安卓的都被设置为不支持立体声
    // Set number of channels
    bool available = false;
    if (adm->StereoPlayoutIsAvailable(&available) != 0) {
      RTC_LOG(LS_ERROR) << "Failed to query stereo playout.";
    }
    if (adm->SetStereoPlayout(available) != 0) {
      RTC_LOG(LS_ERROR) << "Failed to set stereo playout mode.";
    }
  }

  // Recording device.
  {
    // 空实现，也是在 Java 中设置了。
    if (adm->SetRecordingDevice(AUDIO_DEVICE_ID) != 0) {
      RTC_LOG(LS_ERROR) << "Unable to set recording device.";
      return;
    }
    // Init() 已经设置过，返回 0
    if (adm->InitMicrophone() != 0) {
      RTC_LOG(LS_ERROR) << "Unable to access microphone.";
    }

    // 安卓默认为 false
    // Set number of channels
    bool available = false;
    if (adm->StereoRecordingIsAvailable(&available) != 0) {
      RTC_LOG(LS_ERROR) << "Failed to query stereo recording.";
    }
    if (adm->SetStereoRecording(available) != 0) {
      RTC_LOG(LS_ERROR) << "Failed to set stereo recording mode.";
    }
  }
}
```
```C++
// src/sdk/android/src/jni/audio_device/audio_device_module.cc
  int32_t Init() override {
    RTC_DLOG(LS_INFO) << __FUNCTION__;
    RTC_DCHECK(thread_checker_.IsCurrent());
    audio_device_buffer_ =
        std::make_unique<AudioDeviceBuffer>(task_queue_factory_.get());
    // input/output 绑定同一个 audio_device_buffer_。
    AttachAudioBuffer();
    if (initialized_) {
      return 0;
    }
    InitStatus status;
    if (output_->Init() != 0) {
      status = InitStatus::PLAYOUT_ERROR;
    } else if (input_->Init() != 0) {
      output_->Terminate();
      status = InitStatus::RECORDING_ERROR;
    } else {
      initialized_ = true;
      status = InitStatus::OK;
    }
    RTC_HISTOGRAM_ENUMERATION("WebRTC.Audio.InitializationResult",
                              static_cast<int>(status),
                              static_cast<int>(InitStatus::NUM_STATUSES));
    if (status != InitStatus::OK) {
      RTC_LOG(LS_ERROR) << "Audio device initialization failed.";
      return -1;
    }
    return 0;
  }
```

完成以上的初始化之后，就可以开启录音或者播放了。

## Audio 设备状态控制

`WebRtcVoiceEngine` also owns an [`AudioState`][30] member and this class is used has helper to start and stop audio to and from the ADM. To initialize and start recording, it calls:

*   [`AudiDeviceModule::InitRecording`][31]
*   [`AudiDeviceModule::StartRecording`][32]

and to initialize and start playout:

*   [`AudiDeviceModule::InitPlayout`][33]
*   [`AudiDeviceModule::StartPlayout`][34]

Finally, the corresponding stop methods [`AudiDeviceModule::StopRecording`][35]
and [`AudiDeviceModule::StopPlayout`][36] are called followed by
[`AudiDeviceModule::Terminate`][37].

```
SdpOfferAnswerHandler::ApplyLocalDescription
↓
SdpOfferAnswerHandler::UpdateSessionState
↓
SdpOfferAnswerHandler::PushdownMediaDescription(
    SdpType type,
    cricket::ContentSource source,
    const std::map<std::string, const cricket::ContentGroup*>&
        bundle_groups_by_mid)
↓
BaseChannel::SetLocalContent
↓
VoiceChannel::SetLocalContent_w
↓
BaseChannel::UpdateLocalStreams_w
↓
WebRtcVoiceMediaChannel::AddSendStream
↓
WebRtcVoiceMediaChannel::WebRtcAudioSendStream::UpdateSendState()
↓
SendAudioStream::Start()
↓
AudioSendStream::Start() / AudioSendStream::StoreEncoderProperties(int sample_rate_hz,
                                             size_t num_channels)
↓
AudioState::AddSendingStream
```