# Record

WebRtcVoiceEngine 中的 AudioState 调用了 AudiDeviceModule 的 `InitRecording` 和 `StartRecording` 函数后，就开发采集数据发送了。

这里仅关心 JNI 实现方式。

## AudioRecordJni

AudioRecordJni 和 WebRtcAudioRecord.java 互为映射类。`WebRtcAudioRecord::AudioRecordThread.run()` 负责将采集的数据传给 `Transport`。

```Java
// sdk/android/src/java/org/webrtc/audio/WebRtcAudioRecord.java
@Override
public void run() {
  while (keepAlive) {
    // 读取数据
    int bytesRead = audioRecord.read(byteBuffer, byteBuffer.capacity());
    if (bytesRead == byteBuffer.capacity()) {
      if (microphoneMute) {
        byteBuffer.clear();
        byteBuffer.put(emptyBytes);
      }
      // It's possible we've been shut down during the read, and stopRecording() tried and
      // failed to join this thread. To be a bit safer, try to avoid calling any native methods
      // in case they've been unregistered after stopRecording() returned.
      if (keepAlive) {
        ...
        // 发送数据
        nativeDataIsRecorded(nativeAudioRecord, bytesRead, captureTimeNs);
      }
      // 如果数据还有其它使用，可以注册一个回调。例如本地存储。
      if (audioSamplesReadyCallback != null) {
        // Copy the entire byte buffer array. The start of the byteBuffer is not necessarily
        // at index 0.
        byte[] data = Arrays.copyOfRange(byteBuffer.array(), byteBuffer.arrayOffset(),
            byteBuffer.capacity() + byteBuffer.arrayOffset());
        audioSamplesReadyCallback.onWebRtcAudioRecordSamplesReady(
            new JavaAudioDeviceModule.AudioSamples(audioRecord.getAudioFormat(),
                  audioRecord.getChannelCount(), audioRecord.getSampleRate(), data));
        }
      } else {
        ...
      }
    }
    ...
  }
  ...
}
```

`nativeDataIsRecorded` 对应 `AudioRecordJni::DataIsRecorded`

```C++
void AudioRecordJni::DataIsRecorded(JNIEnv* env,
                                    const JavaParamRef<jobject>& j_caller,
                                    int length,
                                    int64_t capture_timestamp_ns) {
  ...
  // direct_buffer_address_ 就是 Java 中 byteBuffer 的地址。可以从中获取到数据。
  audio_device_buffer_->SetRecordedBuffer(
      direct_buffer_address_, frames_per_buffer_, capture_timestamp_ns);
  // We provide one (combined) fixed delay estimate for the APM and use the
  // `playDelayMs` parameter only. Components like the AEC only sees the sum
  // of `playDelayMs` and `recDelayMs`, hence the distributions does not matter.
  audio_device_buffer_->SetVQEData(total_delay_ms_, 0);
  // audio_device_buffer_ 实际发送数据
  if (audio_device_buffer_->DeliverRecordedData() == -1) {
    RTC_LOG(LS_INFO) << "AudioDeviceBuffer::DeliverRecordedData failed";
  }
}
```

```C++
// modules/audio_device/audio_device_buffer.cc
int32_t AudioDeviceBuffer::DeliverRecordedData() {
  if (!audio_transport_cb_) {
    RTC_LOG(LS_WARNING) << "Invalid audio transport";
    return 0;
  }
  const size_t frames = rec_buffer_.size() / rec_channels_;
  const size_t bytes_per_frame = rec_channels_ * sizeof(int16_t);
  uint32_t new_mic_level_dummy = 0;
  uint32_t total_delay_ms = play_delay_ms_ + rec_delay_ms_;
  // AudioTransport 向下传递数据。
  int32_t res = audio_transport_cb_->RecordedDataIsAvailable(
      rec_buffer_.data(), frames, bytes_per_frame, rec_channels_,
      rec_sample_rate_, total_delay_ms, 0, 0, typing_status_,
      new_mic_level_dummy, capture_timestamp_ns_);
  if (res == -1) {
    RTC_LOG(LS_ERROR) << "RecordedDataIsAvailable() failed";
  }
  return 0;
}
```
`audio_transport_cb_` 实际是 `AudioTransportImpl`

到这里数据就传给 `AudioTransport` 了，剩下的就是给 Audio 的处理和编解码了。
